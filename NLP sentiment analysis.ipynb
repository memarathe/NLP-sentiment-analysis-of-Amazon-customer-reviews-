{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa8f1c65",
   "metadata": {},
   "source": [
    "## Stentiment analysis for Amazon product reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "miniature-respondent",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import string\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-feedback",
   "metadata": {},
   "source": [
    "Since we are working with textual input data, we need to convert the data over to a pandas dataframe from the current format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "important-complexity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.\\t</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.\\t</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.\\t</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews sentiment\n",
       "0  So there is no way for me to plug it in here i...       neg\n",
       "1                      Good case, Excellent value.\\t       pos\n",
       "2                           Great for the jawbone.\\t       pos\n",
       "3  Tied to charger for conversations lasting more...       neg\n",
       "4                                The mic is great.\\t       pos"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"./sentiment-labelled-sentences/sentiment labelled sentences/amazon_cells_labelled.txt\"\n",
    "f = open(path, \"r\")\n",
    "\n",
    "data =[]\n",
    "# Converting it to pandas dataframe\n",
    "for line in f:\n",
    "    review = line[:len(line) - 2]\n",
    "    sentiment = \"neg\" if line[len(line)-2] == \"0\" else \"pos\"\n",
    "    row = [review, sentiment]\n",
    "    data.append(row)\n",
    "\n",
    "df = pd.DataFrame(data, columns = ['reviews', 'sentiment'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ba84e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-share",
   "metadata": {},
   "source": [
    "With our dataframe made, we now need to clean it before analyzing. Apply the `remove_punctuation()` and `remove_stopwords()` functions on our dataset to clean it. This reduces the size of the data and thus helps in faster operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "knowing-murray",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>way plug us unless go converter</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>good case excellent value</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>great jawbone</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tied charger conversations lasting 45 minutesm...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mic great</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews sentiment\n",
       "0                    way plug us unless go converter       neg\n",
       "1                          good case excellent value       pos\n",
       "2                                      great jawbone       pos\n",
       "3  tied charger conversations lasting 45 minutesm...       neg\n",
       "4                                          mic great       pos"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Uncomment the line below if you need to download the stopwords\n",
    "#nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('','', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = [word.lower() for word in text.split() if word.lower() not in stop]\n",
    "    return \" \".join(text)\n",
    "\n",
    "df['reviews'] = df['reviews'].apply(remove_punctuation).apply(remove_stopwords)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-aviation",
   "metadata": {},
   "source": [
    "We need to adjust our data slightly before using LDA. In the cell below, use the `CountVectorizer()` function. Then, use `fit_transform()` with `df['reviews']` as a parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "lasting-squad",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(max_features = 5000, max_df=.15)\n",
    "X = vect.fit_transform(df['reviews'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-gross",
   "metadata": {},
   "source": [
    "Using the `LatenDirichletAllocation()` function below, we want to pass it 10 components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "quantitative-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=10, learning_method=\"batch\", max_iter=25, random_state=0) \n",
    "document_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92036b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1788)\n"
     ]
    }
   ],
   "source": [
    "print(lda.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fc3ca58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.87141694, 0.0142878 , 0.01428764, ..., 0.01428817, 0.01428655,\n",
       "        0.01428601],\n",
       "       [0.02000096, 0.0200025 , 0.02000158, ..., 0.02000055, 0.02000212,\n",
       "        0.81998241],\n",
       "       [0.03334133, 0.03333333, 0.03333919, ..., 0.03333333, 0.03333333,\n",
       "        0.29768756],\n",
       "       ...,\n",
       "       [0.02500393, 0.02500102, 0.025     , ..., 0.02500125, 0.025001  ,\n",
       "        0.025005  ],\n",
       "       [0.01428746, 0.01428571, 0.01428599, ..., 0.01428603, 0.01428745,\n",
       "        0.87142414],\n",
       "       [0.21770012, 0.01666792, 0.01666897, ..., 0.01666814, 0.01667184,\n",
       "        0.01666977]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-answer",
   "metadata": {},
   "source": [
    "And finally' let's see the results! Call the `print_topics()` function below, passing in `feature_names` and `sorting`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f57e38cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[[1767  697  717 ...  668 1592 1392]\n",
      " [1469 1261 1238 ... 1072 1592 1392]\n",
      " [ 697 1667 1274 ...  515  668 1392]\n",
      " ...\n",
      " [ 461  998 1709 ... 1592 1524 1392]\n",
      " [1764  893  439 ...  515 1072 1392]\n",
      " [ 697  547 1238 ... 1592  515 1392]]\n"
     ]
    }
   ],
   "source": [
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "print(len(sorting))\n",
    "print(sorting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c80501d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1788\n",
      "['10' '100' '11' ... 'youll' 'z500a' 'zero']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(vect.get_feature_names_out())\n",
    "print(len(feature_names))\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e64ed46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4\n",
      "0 1 2 3 4\n",
      "topic 0        topic 1        topic 2        topic 3        topic 4       \n",
      "--------       --------       --------       --------       --------      \n",
      "works          sound          great          recommend      battery       \n",
      "great          really         use            would          good          \n",
      "happy          quality        reception      service        horrible      \n",
      "easy           good           make           customer       software      \n",
      "battery        headset        car            one            life          \n",
      "junk           product        like           highly         also          \n",
      "use            bad            new            ear            product       \n",
      "piece          well           working        right          cell          \n",
      "cheap          bluetooth      product        stay           never         \n",
      "item           service        light          terrible       nice          \n",
      "\n",
      "\n",
      "5 6 7 8 9\n",
      "5 6 7 8 9\n",
      "topic 5        topic 6        topic 7        topic 8        topic 9       \n",
      "--------       --------       --------       --------       --------      \n",
      "use            ive            dont           work           great         \n",
      "design         best           money          like           excellent     \n",
      "good           ever           waste          disappointed   quality       \n",
      "think          ear            buy            charger        price         \n",
      "money          one            product        well           good          \n",
      "work           worst          everything     time           love          \n",
      "case           bought         good           worked         product       \n",
      "awesome        long           within         im             headset       \n",
      "problems       purchase       broke          new            poor          \n",
      "volume         well           one            motorola       sound         \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_topics(topics, feature_names, sorting, topics_per_chunk, n_words):\n",
    "    for i in range(0, len(topics), topics_per_chunk):\n",
    "        chunk_indices = range(i, min(i + topics_per_chunk, len(topics)))\n",
    "        print(*chunk_indices)\n",
    "        print(' '.join(map(str, chunk_indices)))\n",
    "        print(' '.join('{:<14}'.format('topic ' + str(i)) for i in chunk_indices))\n",
    "        print(' '.join('{:<14}'.format('--------') for _ in chunk_indices))\n",
    "\n",
    "        for j in range(n_words):\n",
    "            words = []\n",
    "            for topic_idx in chunk_indices:\n",
    "                words.append(feature_names[sorting[topic_idx, j]])\n",
    "            print(' '.join('{:<14}'.format(word) for word in words))\n",
    "        print('\\n')\n",
    "print_topics(topics=range(10), feature_names=feature_names, sorting=sorting, topics_per_chunk=5, n_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb089465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ca28af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "path1 = \"./sentiment-labelled-sentences/sentiment labelled sentences/amazon_cells_labelled.txt\"\n",
    "path2 = \"./sentiment-labelled-sentences/sentiment labelled sentences/imdb_labelled.txt\"\n",
    "path3 = \"./sentiment-labelled-sentences/sentiment labelled sentences/yelp_labelled.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae7b991",
   "metadata": {},
   "source": [
    "Lemmatization is the process of reducing a word to its base or root form, called a lemma, by removing suffixes and prefixes. Unlike stemming, lemmatization ensures that the resulting word is a valid word in the language, often using a vocabulary and morphological analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31b85c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "lemmer = WordNetLemmatizer()\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cab64f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean the data\n",
    "sent_tokens = []\n",
    "\n",
    "# Read data from files and tokenize\n",
    "for path in [path1, path2, path3]:\n",
    "    with open(path, \"r\") as file:\n",
    "        for line in file:\n",
    "            review = line.strip()[:-2]  # Remove trailing newline and sentiment label\n",
    "            sent_tokens.append(LemNormalize(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "195f95c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default greeting messages\n",
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\", \"hey\")\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "\n",
    "def greeting(sentence):\n",
    "    # Check if any greeting word is present in the user's input\n",
    "    for word in GREETING_INPUTS:\n",
    "        if word.lower() in sentence.lower():\n",
    "            return random.choice(GREETING_RESPONSES)\n",
    "    return None  # No greeting word found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8ddf6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type 'bye'!\n",
      "ROBO: not even a hello we will be right with you\n",
      "ROBO: Bye! take care..\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Implement the chatbot\n",
    "def response(user_response):\n",
    "    robo_response = ''\n",
    "    sent_tokens.append(LemNormalize(user_response))\n",
    "    tfidfvec = TfidfVectorizer(tokenizer=lambda x: x, lowercase=False, stop_words='english')\n",
    "    tfidf = tfidfvec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx = vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    similarity = flat[-2]\n",
    "    if similarity == 0:\n",
    "        robo_response = robo_response + \"I am sorry! I don't understand you\"\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response + ' '.join(sent_tokens[idx])\n",
    "        return robo_response\n",
    "\n",
    "# Initialize the chatbot\n",
    "flag = True\n",
    "print(\"ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type 'bye'!\")\n",
    "\n",
    "while flag:\n",
    "    user_response = input()\n",
    "    user_response = user_response.lower()\n",
    "    if user_response != 'bye':\n",
    "        if user_response == 'thanks' or user_response == 'thank you':\n",
    "            flag = False\n",
    "            print(\"ROBO: You are welcome..\")\n",
    "        else:\n",
    "            print(\"ROBO: \", end=\"\")\n",
    "            print(response(user_response))\n",
    "            sent_tokens.remove(LemNormalize(user_response))\n",
    "    else:\n",
    "        flag = False\n",
    "        print(\"ROBO: Bye! take care..\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
